{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPraEnh6+aWGNNp4rvKE74K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"n0dr7lurUzZ2"},"outputs":[],"source":["# !pip install --upgrade datasets fsspec\n","from datasets import load_dataset\n","\n","ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")"]},{"cell_type":"code","source":["from datasets import load_dataset\n","from collections import Counter\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Load dataset\n","ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n","\n","# Build vocabulary from training set\n","train_text = \" \".join([text for text in ds[\"train\"][\"text\"] if text.strip() != \"\"])\n","words = train_text.split()\n","word_freq = Counter(words)\n","vocab_size = 10000\n","vocab = [\"<PAD>\", \"<UNK>\"] + [word for word, _ in word_freq.most_common(vocab_size - 2)]\n","word_to_index = {word: idx for idx, word in enumerate(vocab)}\n","\n","pad_token_id = word_to_index[\"<PAD>\"]\n","unk_token_id = word_to_index[\"<UNK>\"]\n","\n","# Tokenization function\n","def tokenize(text):\n","    return [word_to_index.get(word, unk_token_id) for word in text.split()]\n","\n","# Preprocess a dataset split into sequences\n","def preprocess_split(split):\n","    tokenized = [tokenize(text) for text in split[\"text\"] if text.strip() != \"\"]\n","    all_tokens = [token for seq in tokenized for token in seq]\n","    seq_len = 128\n","    sequences = [all_tokens[i:i + seq_len] for i in range(0, len(all_tokens), seq_len)]\n","    # Pad the last sequence\n","    if len(sequences[-1]) < seq_len:\n","        sequences[-1] += [pad_token_id] * (seq_len - len(sequences[-1]))\n","    return sequences\n","\n","# Process all splits\n","train_sequences = preprocess_split(ds[\"train\"])\n","val_sequences = preprocess_split(ds[\"validation\"])\n","test_sequences = preprocess_split(ds[\"test\"])\n","\n","# Custom dataset class\n","class WikiTextDataset(Dataset):\n","    def __init__(self, sequences):\n","        self.sequences = sequences\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.sequences[idx], dtype=torch.long)\n","\n","# Create data loaders\n","batch_size = 32\n","train_dataset = WikiTextDataset(train_sequences)\n","val_dataset = WikiTextDataset(val_sequences)\n","test_dataset = WikiTextDataset(test_sequences)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)"],"metadata":{"id":"aVZ_Ca5XU3ut"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install --upgrade datasets fsspec\n","from datasets import load_dataset\n","from collections import Counter\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import torch.nn as nn\n","import math\n","\n","# Load dataset\n","ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n","\n","# Build vocabulary from training set\n","train_text = \" \".join([text for text in ds[\"train\"][\"text\"] if text.strip() != \"\"])\n","words = train_text.split()\n","word_freq = Counter(words)\n","vocab_size = 10000\n","vocab = [\"<PAD>\", \"<UNK>\"] + [word for word, _ in word_freq.most_common(vocab_size - 2)]\n","word_to_index = {word: idx for idx, word in enumerate(vocab)}\n","\n","pad_token_id = word_to_index[\"<PAD>\"]\n","unk_token_id = word_to_index[\"<UNK>\"]\n","\n","# Tokenization function\n","def tokenize(text):\n","    return [word_to_index.get(word, unk_token_id) for word in text.split()]\n","\n","# Preprocess a dataset split into sequences\n","def preprocess_split(split):\n","    tokenized = [tokenize(text) for text in split[\"text\"] if text.strip() != \"\"]\n","    all_tokens = [token for seq in tokenized for token in seq]\n","    seq_len = 128\n","    sequences = [all_tokens[i:i + seq_len] for i in range(0, len(all_tokens), seq_len)]\n","    # Pad the last sequence\n","    if len(sequences[-1]) < seq_len:\n","        sequences[-1] += [pad_token_id] * (seq_len - len(sequences[-1]))\n","    return sequences\n","\n","# Process all splits\n","train_sequences = preprocess_split(ds[\"train\"])\n","val_sequences = preprocess_split(ds[\"validation\"])\n","test_sequences = preprocess_split(ds[\"test\"])\n","\n","# Custom dataset class\n","class WikiTextDataset(Dataset):\n","    def __init__(self, sequences):\n","        self.sequences = sequences\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.sequences[idx], dtype=torch.long)\n","\n","# Create data loaders\n","batch_size = 32\n","train_dataset = WikiTextDataset(train_sequences)\n","val_dataset = WikiTextDataset(val_sequences)\n","test_dataset = WikiTextDataset(test_sequences)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","# Define PositionalEncoding class\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_seq_len):\n","        super().__init__()\n","        pe = torch.zeros(max_seq_len, d_model)\n","        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return x\n","\n","# Define MultiHeadAttention class\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads, dropout):\n","        super().__init__()\n","        assert d_model % num_heads == 0\n","        self.num_heads = num_heads\n","        self.d_k = d_model // num_heads\n","        self.WQ = nn.Linear(d_model, d_model)\n","        self.WK = nn.Linear(d_model, d_model)\n","        self.WV = nn.Linear(d_model, d_model)\n","        self.WO = nn.Linear(d_model, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, X, mask):\n","        batch_size, seq_len, _ = X.size()\n","        Q = self.WQ(X).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n","        K = self.WK(X).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n","        V = self.WV(X).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n","        scores = scores + mask\n","        weights = torch.nn.functional.softmax(scores, dim=-1)\n","        weights = self.dropout(weights)\n","        output = torch.matmul(weights, V)\n","        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n","        output = self.WO(output)\n","        return output\n","\n","# Define FeedForward class\n","class FeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff, dropout):\n","        super().__init__()\n","        self.linear1 = nn.Linear(d_model, d_ff)\n","        self.linear2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = torch.nn.functional.relu(self.linear1(x))\n","        x = self.dropout(x)\n","        x = self.linear2(x)\n","        return x\n","\n","# Define TransformerLayer class\n","class TransformerLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout):\n","        super().__init__()\n","        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n","        self.ffn = FeedForward(d_model, d_ff, dropout)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        attn_input = self.norm1(x)\n","        attn_output = self.attention(attn_input, mask)\n","        x = x + self.dropout(attn_output)\n","        ffn_input = self.norm2(x)\n","        ffn_output = self.ffn(ffn_input)\n","        x = x + self.dropout(ffn_output)\n","        return x\n","\n","# Define Transformer class\n","class Transformer(nn.Module):\n","    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_len, dropout):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n","        self.layers = nn.ModuleList([TransformerLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n","        self.norm = nn.LayerNorm(d_model)\n","        self.output_layer = nn.Linear(d_model, vocab_size)\n","        self.output_layer.weight = self.embedding.weight  # Weight tying\n","        self.pad_token_id = 0\n","\n","    def forward(self, input_ids):\n","        batch_size, seq_len = input_ids.size()\n","        # Create causal mask (triangular) and padding mask\n","        causal_mask = torch.triu(torch.ones((seq_len, seq_len), device=input_ids.device), diagonal=1).bool()\n","        padding_mask = (input_ids == self.pad_token_id)\n","        full_mask = causal_mask[None, None, :, :] | padding_mask[:, None, None, :]\n","        full_mask = full_mask.float() * -1e9  # Masked positions get a large negative value\n","        x = self.embedding(input_ids)\n","        x = self.pos_encoding(x)\n","        for layer in self.layers:\n","            x = layer(x, full_mask)\n","        x = self.norm(x)\n","        logits = self.output_layer(x)\n","        return logits\n","\n","# Set device (GPU if available, else CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize the model\n","model = Transformer(\n","    vocab_size=vocab_size, # Use actual vocab_size from data preprocessing\n","    d_model=256,\n","    num_layers=6,\n","    num_heads=8,\n","    d_ff=1024,\n","    max_seq_len=128, # Use actual seq_len from data preprocessing\n","    dropout=0.1\n",").to(device)\n","\n","# Define optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n","\n","# Training loop with output\n","for epoch in range(3):  # Reduced to 3 epochs for quick demonstration\n","    model.train()\n","    total_loss = 0\n","    for batch in train_loader:\n","        input_ids = batch.to(device)\n","        optimizer.zero_grad()\n","        logits = model(input_ids)\n","        targets = input_ids[:, 1:]  # Shifted target for next-token prediction\n","        logits = logits[:, :-1, :]  # Remove last prediction to match target length\n","        mask = (targets != pad_token_id)  # Mask for padding (use pad_token_id)\n","        # Add .contiguous() before .view() to ensure contiguous memory layout\n","        # Add .contiguous() to targets before calling view()\n","        loss = torch.nn.functional.cross_entropy(logits.contiguous().view(-1, vocab_size), targets.contiguous().view(-1), reduction='none')\n","        # Apply mask to loss and calculate mean\n","        loss = (loss * mask.view(-1)).sum() / mask.sum()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    avg_train_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}\")"],"metadata":{"id":"zSUra-kwU37X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate on test set\n","model.eval()\n","test_loss = 0\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids = batch.to(device)\n","        logits = model(input_ids)\n","        targets = input_ids[:, 1:]  # Shifted target for next-token prediction\n","        logits = logits[:, :-1, :]  # Remove last prediction to match target length\n","        mask = (targets != pad_token_id)  # Mask for padding\n","        loss = torch.nn.functional.cross_entropy(\n","            logits.contiguous().view(-1, vocab_size),\n","            targets.contiguous().view(-1),\n","            reduction='none'\n","        )\n","        loss = (loss * mask.view(-1)).sum() / mask.sum()\n","        test_loss += loss.item()\n","avg_test_loss = test_loss / len(test_loader)\n","perplexity = math.exp(avg_test_loss)\n","print(f\"Test Loss: {avg_test_loss:.4f}, Test Perplexity: {perplexity:.4f}\")"],"metadata":{"id":"CcyMqnoHU4Q9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reverse vocabulary for decoding (ID to word)\n","index_to_word = {idx: word for word, idx in word_to_index.items()}\n","\n","def generate_text(model, prompt_ids, max_length=50, sampling=\"greedy\"):\n","    \"\"\"\n","    Generate text starting from prompt_ids.\n","    Args:\n","        model: Trained Transformer model\n","        prompt_ids: List or tensor of token IDs to start generation\n","        max_length: Maximum number of tokens to generate\n","        sampling: \"greedy\" or \"top_k\" (top_k with k=50 for diversity)\n","    Returns:\n","        List of generated token IDs\n","    \"\"\"\n","    model.eval()\n","    if isinstance(prompt_ids, list):\n","        input_ids = torch.tensor([prompt_ids], dtype=torch.long).to(device)\n","    else:\n","        input_ids = prompt_ids.clone().detach().unsqueeze(0).to(device)\n","\n","    generated = input_ids.tolist()[0]\n","\n","    with torch.no_grad():\n","        for _ in range(max_length):\n","            logits = model(input_ids)\n","            next_logits = logits[:, -1, :]  # Logits for the last position\n","\n","            if sampling == \"greedy\":\n","                next_token = torch.argmax(next_logits, dim=-1)\n","            elif sampling == \"top_k\":\n","                top_k = 50\n","                top_k_probs, top_k_indices = torch.topk(next_logits, top_k, dim=-1)\n","                next_token = top_k_indices[\n","                    0, torch.multinomial(torch.nn.functional.softmax(top_k_probs, dim=-1), 1)\n","                ]\n","\n","            generated.append(next_token.item())\n","            input_ids = torch.tensor([generated], dtype=torch.long).to(device)\n","\n","            if next_token.item() == pad_token_id:\n","                break\n","\n","    return generated\n","\n","# Example: Generate text from a prompt\n","prompt_text = \"The history of\"  # Example prompt\n","prompt_ids = tokenize(prompt_text)[:128]  # Tokenize and truncate to max_seq_len\n","generated_ids = generate_text(model, prompt_ids, max_length=50, sampling=\"top_k\")\n","\n","# Decode generated IDs to text\n","generated_text = \" \".join([index_to_word.get(id, \"<UNK>\") for id in generated_ids])\n","print(\"Generated Text:\", generated_text)"],"metadata":{"id":"hJ30PgBKU4ft"},"execution_count":null,"outputs":[]}]}